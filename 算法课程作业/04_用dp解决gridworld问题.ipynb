{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld问题（Reinforcing learning-Introduction）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 定义给定随机分布，策略评估的算法\n",
    "- **迭代公式：$v_{k+1}(s)=\\sum_{a \\in A} \\pi(a | s)\\left(R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P_{s s^{\\prime}}^{a} v_{k}\\left(s^{\\prime}\\right)\\right)\\quad$(1)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyEvaluation:\n",
    "    def __init__(self,gamma):\n",
    "        self.gamma = gamma          #衰减系数\n",
    "    '''\n",
    "        更新一个状态的价值函数(私有成员)\n",
    "        i，j:指定状态位置\n",
    "        policy:策略\n",
    "        stateValue:状态价值\n",
    "    '''\n",
    "    def __calcaluate_one_state_value(self,i,j,policy,stateValue):\n",
    "        reward = -1                                            #所有动作的即时奖励\n",
    "        value = np.zeros((4,1),dtype=np.float64)\n",
    "        direction = np.array([1,0,-1,0,0,-1,0,1]).reshape(4,2) #定义四个方向：上下左右    \n",
    "        #得到所有可能的状态价值\n",
    "        num = 0\n",
    "        for dir1 in direction:                                #遍历该状态的上下左右四个状态\n",
    "            tempi = i + dir1[0]\n",
    "            tempj = j + dir1[1]\n",
    "            if tempi >= 0 and tempi <= 3 and tempj >= 0 and tempj <= 3:\n",
    "                value[num] = stateValue[tempi,tempj]\n",
    "            else:                                             #边界格往外走，直接回到原本的格子（状态）\n",
    "                value[num] = stateValue[i,j]\n",
    "            num = num+1\n",
    "        \n",
    "        #根据公式(1),计算状态价值\n",
    "        assert(value.shape == (4,1))\n",
    "        \n",
    "        res = np.dot( policy[i,j,:].reshape(1,4), self.gamma*value+ reward )\n",
    "        return res.squeeze()                                          #返回计算出值      \n",
    "    '''\n",
    "    Iterative Policy Evaluation\n",
    "    利用动态规划的方法在给定策略的情况下求出状态价值：\n",
    "    policy:策略\n",
    "    stateValue:初始的状态价值\n",
    "    iterNum:迭代次数 \n",
    "    '''\n",
    "    def policyEvaluation(self,policy,stateValue,iterNum = 1):\n",
    "        \n",
    "        init_value = stateValue.copy()\n",
    "        newStateValue = stateValue.copy()     #新的状态价值\n",
    "        \n",
    "        for num in range(iterNum):           #迭代次数\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    if i == 0 and j == 0:\n",
    "                        continue\n",
    "                    elif i == 3 and j == 3:\n",
    "                        continue\n",
    "                    else:\n",
    "                        newStateValue[i,j] = self.__calcaluate_one_state_value(i,j,policy,init_value)\n",
    "            init_value = newStateValue.copy()     \n",
    "        return newStateValue          #返回经过迭代的价值函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 定义参数\n",
    "- **衰减因子$\\gamma = 1$ **\n",
    "- **所有状态得到的即时奖励都是$R=-1$ **\n",
    "- **每个格子都是固定的，所以可行的状态转换概率$P_{ss'}^{a}=1$(状态s采取动作a转移到一个状态s'的概率)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "policy = np.zeros((4,4,4),dtype = np.float64)  #存放策略\n",
    "#初始化策略\n",
    "assert(policy.shape == (4,4,4))               #确保存放策略数组的维度,四个方向的概率值an'zha\n",
    "policy = policy+0.25                           #利用Python机制让每个格子向上下左anzha右转移的概率为0.25\n",
    "policy[0,0,:] = 0\n",
    "policy[3,3,:] = 0\n",
    "stateValue = np.zeros((4,4),dtype=np.float64)  #用于存储状态价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 运行算法计算给定随机策略的最终状态价值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000 -14.0000 -20.0000 -22.0000]\n",
      " [-14.0000 -18.0000 -20.0000 -20.0000]\n",
      " [-20.0000 -20.0000 -18.0000 -14.0000]\n",
      " [-22.0000 -20.0000 -14.0000 0.0000]]\n"
     ]
    }
   ],
   "source": [
    "Algorithm = PolicyEvaluation(gamma)\n",
    "iterNum = 1000\n",
    "newStateValue = Algorithm.policyEvaluation(policy,stateValue,iterNum)\n",
    "np.set_printoptions(formatter={'float':'{:.4f}'.format})\n",
    "print(newStateValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 当迭代次数k = 1000,状态价值就已经收敛了，其精确到小数点4位的状态价值如上所示，与PPT给出结果一致 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP:Optimal Gridword问题\n",
    "- ** 当采取动作偏离地图范围时，回到原来格子，即时奖励是-1**\n",
    "- ** 状态A采取任何动作都转移到状态A'，得到的即时奖励是+10**\n",
    "- ** 状态B采取任何动作都转移到状态B',得到的即是奖励是+5**\n",
    "- ** 其余的动作，得到的即时奖励是0**\n",
    "- ** 衰减系数$\\gamma = 0.9$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1:定义求取最优策略的算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BestPolicy:\n",
    "    def __init__(self,gamma):\n",
    "        self.gamma = gamma          #衰减系数\n",
    "\n",
    "    def __calculate_one_state_value(self,i,j,policy,stateValue):\n",
    "        reward = np.zeros((4,1),dtype=np.float64)               #默认每个状态的四个动作即时奖励为0\n",
    "        value = np.zeros((4,1),dtype=np.float64)            \n",
    "        direction = np.array([1,0,-1,0,0,-1,0,1]).reshape(4,2) #定义四个方向，顺序为下上左右    \n",
    "        num = 0\n",
    "        for dir1 in direction:                                #遍历该状态的下上左右四个状态\n",
    "            \n",
    "            tempi = i + dir1[0]                                #i是行，j是列\n",
    "            tempj = j + dir1[1]\n",
    "            if tempi >= 0 and tempi <= 4 and tempj >= 0 and tempj <= 4:\n",
    "                value[num] = stateValue[tempi,tempj]\n",
    "            else:                                             #边界格往外走，直接回到原本的格子（状态）\n",
    "                value[num] = stateValue[i,j]\n",
    "                if tempi < 0 :   #上越界\n",
    "                    reward[0] = -1\n",
    "                elif tempi > 4:  #下越界\n",
    "                    reward[1] = -1\n",
    "                elif tempj < 0:  #左越界\n",
    "                    reward[2] = -1\n",
    "                elif tempj > 4:  #右越界\n",
    "                    reward[3] = -1      \n",
    "            num = num+1\n",
    "        \n",
    "        #根据公式(1),计算状态价值\n",
    "        \n",
    "        if i == 0 and j == 1:           #当前是状态A\n",
    "            reward = np.full((4,1),10)\n",
    "            value = np.full((4,1),stateValue[4,2])\n",
    "        elif i == 0 and j == 3:         #当前是状态B\n",
    "            reward = np.full((4,1),5)\n",
    "            value = np.full((4,1),stateValue[2,3])\n",
    "        \n",
    "        #print(i,j,reward)\n",
    "        assert(value.shape == (4,1))\n",
    "        assert(reward.shape == (4,1))\n",
    "        \n",
    "        res = np.dot(policy[i,j,:].reshape(1,4), self.gamma*value+ reward )\n",
    "        return res.squeeze()                                          #返回计算出值      \n",
    "    \n",
    "    def policyEvaluation(self,policy,stateValue,iterNum = 1):\n",
    "        \n",
    "        init_value = stateValue.copy()\n",
    "        newStateValue = stateValue.copy()     #新的状态价值\n",
    "        \n",
    "        for num in range(iterNum):           #迭代次数\n",
    "            for i in range(5):\n",
    "                for j in range(5):\n",
    "                    newStateValue[i,j] = self.__calculate_one_state_value(i,j,policy,init_value)\n",
    "            init_value = newStateValue.copy()   \n",
    "        return newStateValue          #返回经过迭代的价值函数\n",
    "    \n",
    "    #计算一个状态的概略分布\n",
    "    def __calculatePolicy(self,i,j,stateValue): \n",
    "        maxv = 0.\n",
    "        curr_value = np.zeros((4,1),dtype=np.float64)\n",
    "        assert(curr_value.shape == (4,1))\n",
    "        res = np.zeros((4,1),dtype=np.float64)\n",
    "        assert(res.shape == (4,1))\n",
    "        \n",
    "        direction = np.array([1,0,-1,0,0,-1,0,1]).reshape(4,2) #定义四个方向，顺序为下上左右    \n",
    "        num = 0\n",
    "        \n",
    "        for dir1 in direction:                                #遍历该状态的上下左右四个状态\n",
    "            tempi = i + dir1[0]                                #i是行，j是列\n",
    "            tempj = j + dir1[1]\n",
    "            \n",
    "            if tempi >= 0 and tempi <= 4 and tempj >= 0 and tempj <= 4:\n",
    "                curr_value[num] = stateValue[tempi,tempj]\n",
    "            else:\n",
    "                curr_value[num] = stateValue[i,j]\n",
    "                \n",
    "            #边界格往外走，直接回到原本的格子（状态）  \n",
    "            num = num+1        \n",
    "        \n",
    "        maxv = curr_value[0]\n",
    "        for tmp in curr_value:          #找出最大的状态价值函数\n",
    "            if tmp > maxv:\n",
    "                maxv = tmp\n",
    "                \n",
    "       \n",
    "        true_num = 0\n",
    "        flagList = []         \n",
    "        for tmp in curr_value:\n",
    "            if maxv - tmp < 0.1:   #状态价值的数值近似相等\n",
    "                flagList.append(True)\n",
    "                true_num = true_num+1\n",
    "            else:\n",
    "                flagList.append(False)\n",
    "        \n",
    "        #计算四个方向的概略分布\n",
    "        prob = 1.0/true_num      \n",
    "        num = 0\n",
    "        for tmp in flagList:\n",
    "            if tmp == True:\n",
    "                res[num] = prob\n",
    "            else:\n",
    "                res[num] = 0.0\n",
    "            num = num + 1\n",
    "        return res\n",
    "             \n",
    "    def policyImproverment(self,stateValue,policy):\n",
    "        newPolicy = policy.copy()\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                if row == 0 and col == 1:       #状态A的策略不需要更新，已是最优\n",
    "                    continue\n",
    "                elif row == 0 and col == 3:     #状态B的策略不需要更新，已是最优\n",
    "                    continue\n",
    "                else:\n",
    "                    newPolicy[row,col,:] = self.__calculatePolicy(row,col,stateValue).squeeze().copy()\n",
    "        return newPolicy\n",
    "    def print_policy(self,policy):\n",
    "        allList = []\n",
    "        dirList = [\"下\",\"上\",\"左\",\"右\"]\n",
    "        \n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                num = 0\n",
    "                for dir1 in dirList:\n",
    "                    if policy[i,j,num] > 0:\n",
    "                        allList.append(dirList[num])\n",
    "                    else:\n",
    "                        allList.append(\"-\")\n",
    "                    num += 1\n",
    "        #打印方向\n",
    "        num = 0\n",
    "        j = 0\n",
    "        while j < len(allList):\n",
    "            for i in range(j,j+4):\n",
    "                print(allList[i],end = \"\")\n",
    "            print(\" \",end = \"\")\n",
    "            j += 4\n",
    "            num += 1\n",
    "            if num%5 == 0:\n",
    "                print(\"\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.9   #衰减系数\n",
    "policy = np.zeros((5,5,4),dtype = np.float64)  #存放策略\n",
    "#初始化策略\n",
    "assert(policy.shape == (5,5,4))               #确保存放策略数组的维度,四个方向的概率值\n",
    "policy = policy+0.25                           #利用Python机制让每个格子向上下左右转移的概率为0.25\n",
    "stateValue = np.zeros((5,5),dtype=np.float64)  #用于存储状态价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 测试策略评价函数是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.3681 8.8986 4.4684 5.3293 1.4976]\n",
      " [1.5565 3.0344 2.2741 1.9177 0.5535]\n",
      " [0.0695 0.7571 0.6863 0.3659 -0.3978]\n",
      " [-0.9632 -0.4257 -0.3472 -0.5802 -1.1789]\n",
      " [-1.8507 -1.3387 -1.2239 -1.4187 -1.9716]]\n"
     ]
    }
   ],
   "source": [
    "Algorithm1 = BestPolicy(gamma)\n",
    "iterNum = 50\n",
    "newStateValue = Algorithm1.policyEvaluation(policy,stateValue,iterNum)\n",
    "#np.set_printoptions(formatter={'float':'{:.4f}'.format})\n",
    "print(newStateValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4 测试策略提升函数是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin policy:\n",
      "[[[0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]]\n",
      "\n",
      " [[0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]]\n",
      "\n",
      " [[0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]]\n",
      "\n",
      " [[0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]]\n",
      "\n",
      " [[0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.2500 0.2500 0.2500 0.2500]]]\n",
      "---------------------------------\n",
      "new policy:\n",
      "[[[0.0000 0.0000 0.0000 1.0000]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.0000 0.0000 1.0000 0.0000]\n",
      "  [0.2500 0.2500 0.2500 0.2500]\n",
      "  [0.0000 0.0000 1.0000 0.0000]]\n",
      "\n",
      " [[0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 0.0000 1.0000 0.0000]]\n",
      "\n",
      " [[0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]]\n",
      "\n",
      " [[0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]]\n",
      "\n",
      " [[0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000 0.0000]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"origin policy:\")\n",
    "print(policy)\n",
    "print(\"---------------------------------\")\n",
    "newPolicy = Algorithm1.policyImproverment(newStateValue,policy)\n",
    "print(\"new policy:\")\n",
    "print(newPolicy)\n",
    "#print(newPolicy)\n",
    "#Algorithm1.print_policy(newPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5 结合策略评估算法与策略提升算法寻找最优策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test = BestPolicy(gamma)\n",
    "iterNum = 50\n",
    "allNum = 100\n",
    "\n",
    "policy_A = policy.copy()\n",
    "stateValue_A = stateValue.copy()\n",
    "\n",
    "for i in range(allNum):\n",
    "    stateValue_B = Test.policyEvaluation(policy_A,stateValue_A,iterNum)\n",
    "    #stateValue_B = np.round(stateValue_B,decimals=1)\n",
    "    #print(\"stateValue_B：\")\n",
    "    #print(stateValue_B)\n",
    "    policy_B = Test.policyImproverment(stateValue_B,policy_A)\n",
    "    #Test.print_policy(policy_B)\n",
    "    #print(\"--------------------------------------------------------------------------------\")\n",
    "    policy_A = policy_B.copy()\n",
    "    stateValue_A = stateValue_B.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---右 下上左右 --左- 下上左右 --左- \n",
      "-上-右 -上-- -上左- -上-- -上左- \n",
      "-上-右 -上-- -上左- -上-- -上左- \n",
      "-上-右 -上-- -上左- -上-- -上左- \n",
      "-上-- -上-- -上-- -上-- -上-- \n",
      "[[17.4791 19.4212 17.4791 18.4502 16.6052]\n",
      " [15.7312 17.4791 15.7312 16.6052 14.9446]\n",
      " [14.1581 15.7312 14.1581 14.9446 13.4502]\n",
      " [12.7423 14.1581 12.7423 13.4502 12.1052]\n",
      " [10.4681 11.7423 10.4681 11.1052 9.8946]]\n"
     ]
    }
   ],
   "source": [
    "Test.print_policy(policy_B)\n",
    "print(stateValue_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最终得到的策略以及状态价值如上**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
